{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Bunch\n",
    "\n",
    "12 September 2021\n",
    "\n",
    "Bellevue University\n",
    "\n",
    "DSC550-T301\n",
    "\n",
    "---\n",
    "\n",
    "# Week 2 Exercises: Build Your Text Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import sys\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Text:\n",
    "\n",
    "For this part, you will start by reading the controversial-comments.jsonl file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        con                                                txt\n365742    0                 Down vote this dick in your mouth.\n217278    0  I just remembered I was raped by Trump too. I ...\n21252     0        Yalec Taldwin works primarily in Bollywood.\n402752    1  Bbbbut the liberals hate Snowden and Wikileaks...\n258448    0  This sub has gone to shit. I'm only here for t...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>con</th>\n      <th>txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>365742</th>\n      <td>0</td>\n      <td>Down vote this dick in your mouth.</td>\n    </tr>\n    <tr>\n      <th>217278</th>\n      <td>0</td>\n      <td>I just remembered I was raped by Trump too. I ...</td>\n    </tr>\n    <tr>\n      <th>21252</th>\n      <td>0</td>\n      <td>Yalec Taldwin works primarily in Bollywood.</td>\n    </tr>\n    <tr>\n      <th>402752</th>\n      <td>1</td>\n      <td>Bbbbut the liberals hate Snowden and Wikileaks...</td>\n    </tr>\n    <tr>\n      <th>258448</th>\n      <td>0</td>\n      <td>This sub has gone to shit. I'm only here for t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is easily accomplished thanks to pandas read_json() function.\n",
    "# The file appears to be line-delineated, which we can handle by setting the \"lines\" parameter to True.\n",
    "df_raw = pd.read_json(\"controversial-comments.jsonl\", lines=True)\n",
    "\n",
    "# My computer had a difficult time processing the full sample, so I will select a random subset to work with.\n",
    "df = df_raw.sample(n = 50000, random_state=12)\n",
    "\n",
    "# View the first five observations to check for the expected results.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then:\n",
    "\n",
    "A. Convert all text to lowercase letters.\n",
    "\n",
    "B. Remove all punctuation from the text.\n",
    "\n",
    "C. Remove stop words.\n",
    "\n",
    "D. Apply NLTKâ€™s PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# First, I will create a translation table that maps punctuation characters to None values.\n",
    "punc_tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# Load the list of stop words from the nltk library.\n",
    "# nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Create a Stemmer from the nltk PorterStemmer function.\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "def prep_text(t):\n",
    "    \"\"\" This function will convert all text to lowercase letters, remove all punctuation, tokenize the text into words,\n",
    "    remove all stop words, apply the PorterStemmer function to each word, and finally recombine the tokenized words.\"\"\"\n",
    "    # Strip any leading or following white spaces and convert all text to lowercase.\n",
    "    text = t.strip().lower()\n",
    "    # Apply the translation table to the text to remove punctuation characters.\n",
    "    text = text.translate(punc_tbl)\n",
    "    # Next, we will tokenize the text into individual words.\n",
    "    tok_text = nltk.tokenize.word_tokenize(text)\n",
    "    # Create a new list of tokenized words that are NOT stop words.\n",
    "    tok_text_nsw = [word for word in tok_text if word not in stop_words]\n",
    "    # Now we can apply the nltk PorterStemmer function to stem the tokenized words.\n",
    "    tok_text_fin = [porter.stem(word) for word in tok_text_nsw]\n",
    "    # Combine the cleaned and stemmed words back into a string.\n",
    "    text_fin = \" \".join(tok_text_fin)\n",
    "    return text_fin\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "        con                                                txt  \\\n365742    0                 Down vote this dick in your mouth.   \n217278    0  I just remembered I was raped by Trump too. I ...   \n21252     0        Yalec Taldwin works primarily in Bollywood.   \n402752    1  Bbbbut the liberals hate Snowden and Wikileaks...   \n258448    0  This sub has gone to shit. I'm only here for t...   \n\n                                         prepped_text  \\\n365742                                vote dick mouth   \n217278           rememb rape trump rememb 5 day elect   \n21252          yalec taldwin work primarili bollywood   \n402752  bbbbut liber hate snowden wikileak great news   \n258448       sub gone shit im idioci nov 8 unsubscrib   \n\n                                           prepped_tokens  \n365742                                [vote, dick, mouth]  \n217278       [rememb, rape, trump, rememb, 5, day, elect]  \n21252        [yalec, taldwin, work, primarili, bollywood]  \n402752  [bbbbut, liber, hate, snowden, wikileak, great...  \n258448  [sub, gone, shit, im, idioci, nov, 8, unsubscrib]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>con</th>\n      <th>txt</th>\n      <th>prepped_text</th>\n      <th>prepped_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>365742</th>\n      <td>0</td>\n      <td>Down vote this dick in your mouth.</td>\n      <td>vote dick mouth</td>\n      <td>[vote, dick, mouth]</td>\n    </tr>\n    <tr>\n      <th>217278</th>\n      <td>0</td>\n      <td>I just remembered I was raped by Trump too. I ...</td>\n      <td>rememb rape trump rememb 5 day elect</td>\n      <td>[rememb, rape, trump, rememb, 5, day, elect]</td>\n    </tr>\n    <tr>\n      <th>21252</th>\n      <td>0</td>\n      <td>Yalec Taldwin works primarily in Bollywood.</td>\n      <td>yalec taldwin work primarili bollywood</td>\n      <td>[yalec, taldwin, work, primarili, bollywood]</td>\n    </tr>\n    <tr>\n      <th>402752</th>\n      <td>1</td>\n      <td>Bbbbut the liberals hate Snowden and Wikileaks...</td>\n      <td>bbbbut liber hate snowden wikileak great news</td>\n      <td>[bbbbut, liber, hate, snowden, wikileak, great...</td>\n    </tr>\n    <tr>\n      <th>258448</th>\n      <td>0</td>\n      <td>This sub has gone to shit. I'm only here for t...</td>\n      <td>sub gone shit im idioci nov 8 unsubscrib</td>\n      <td>[sub, gone, shit, im, idioci, nov, 8, unsubscrib]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the pre-processing function to the text data and add the results to the table as a new column.\n",
    "df['prepped_text'] = df['txt'].apply(prep_text)\n",
    "\n",
    "# It might also be useful to add the tokenized version of this pre-processed text as another new column.\n",
    "df['prepped_tokens'] = df['prepped_text'].apply(nltk.tokenize.word_tokenize)\n",
    "\n",
    "# View a sample of the results.\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now that the data is pre-processed, you will apply three different techniques to get it into a usable form for model-building. Apply each of the following steps (individually) to the pre-processed data.\n",
    "\n",
    "A. Convert each text entry into a word-count vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a vectorizer from the sklern CountVectorizer() function.\n",
    "count = sklearn.feature_extraction.text.CountVectorizer()\n",
    "\n",
    "# Apply the vectorizer to the prepared text feature.\n",
    "wcv = count.fit_transform(df.prepped_text)\n",
    "\n",
    "# View some details of the matrix.\n",
    "wcv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<50000x32766 sparse matrix of type '<class 'numpy.int64'>'\n\twith 809954 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The matrix is not very interesting to view this way, but it does appear to be the expected result.\n",
    "wcv.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['admir',\n 'admiralti',\n 'admiss',\n 'admissionth',\n 'admit',\n 'admitt',\n 'admittedhttpwwwmotherjonescompolitics201609donaldtrumpliesaboutdealingsmafiafigur',\n 'admittedli',\n 'admonish',\n 'ado',\n 'adolesc',\n 'adolf',\n 'adolph',\n 'adopt',\n 'adopteesor',\n 'ador',\n 'adrenalin',\n 'adress',\n 'adroitli',\n 'adsrath']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View some of the words corresponding to the features.\n",
    "count.get_feature_names()[2000:2020]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B. Convert each text entry into a part-of-speech tag vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will store the part of speech tag for each word in each observation.\n",
    "tagged_list = []\n",
    "\n",
    "# Loop through the tokenized words in each observation, tagging each word with its part of speech.\n",
    "# Then, add the part of speech tags, per each observation, to the tagged_list.\n",
    "for val in df.prepped_tokens.values:\n",
    "    tags = nltk.pos_tag(val)\n",
    "    tagged_list.append([tag for word, tag in tags])\n",
    "\n",
    "# Create a one-hot binarizer from the sklearn MultiLabelBinarizer() function.\n",
    "one_hot_multi = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "\n",
    "# Apply the binarizer to the tagged_list to create one-hot encoded part-of-speech vectors.\n",
    "psv = one_hot_multi.fit_transform(tagged_list)\n",
    "\n",
    "# View the resulting array.\n",
    "psv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['$', \"''\", 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS',\n       'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP',\n       'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD',\n       'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``'],\n      dtype=object)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the corresponding feature names.\n",
    "one_hot_multi.classes_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "C. Convert each entry into a term frequency-inverse document frequency (tfidf) vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<50000x32766 sparse matrix of type '<class 'numpy.float64'>'\n\twith 809954 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tfidf vectorizer.\n",
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "# Apply the vectorizer to create a tfidf feature matrix.\n",
    "tfv = tfidf.fit_transform(df.prepped_text)\n",
    "\n",
    "# View details about the matrix.\n",
    "tfv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the matrix.\n",
    "tfv.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "[('vote', 31386),\n ('dick', 8024),\n ('mouth', 19521),\n ('rememb', 24211),\n ('rape', 23706),\n ('trump', 29656),\n ('day', 7457),\n ('elect', 9075),\n ('yalec', 32481),\n ('taldwin', 28425)]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View some of the words corresponding to the features.\n",
    "list(tfidf.vocabulary_.items())[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Follow-Up Question:\n",
    "\n",
    "For the three techniques in problem (2) above, give an example where each would be useful.\n",
    "\n",
    "1. Creating word-count vectors could be useful for extracting the domain or sentiment of the text.  For example, a high count of domain-specific words might indicate that the text could be classified as part of that domain.\n",
    "2. Creating part-of-speech vectors could be useful for extracting intent from the text.  For example, user text entries could be filtered for a high number of verbs, potentially indicating an intent to act.\n",
    "3. Creating tfidf vectors is useful for quantifying how important different words are in the context of the overall document or set of documents being analyzed.  One example might be analyzing posts on a message board thread to identify \"trolling\", i.e., posts that do not use many (or any) of the terms being used by most of the other posts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}